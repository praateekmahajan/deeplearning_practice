{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/praateek/miniconda3/envs/ml/lib/python3.6/site-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['sample']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F \n",
    "import torch.utils.data\n",
    "import  torch.optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNNDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, text, sequence_length):\n",
    "        self.char_ls = sorted(list(set(text)))\n",
    "        self.vocab_size = len(self.char_ls)\n",
    "        self.char_to_int_map = dict(zip(self.char_ls, range(len(self.char_ls))))\n",
    "        self.int_to_char_map = dict(zip(range(len(self.char_ls)), self.char_ls))\n",
    "        full_data = np.array(list(map(self.char_to_int_map.get, text)))\n",
    "        # Truncate slightly so we have a num_chars = num_lines * seq_length\n",
    "        print(\"Full data\", len(full_data))\n",
    "\n",
    "        self.num_lines = int(len(full_data) / sequence_length)\n",
    "        print(\"Num lines\", self.num_lines)\n",
    "        print(\"Num lines\", self.num_lines * sequence_length)\n",
    "\n",
    "        full_data = full_data[\n",
    "            :self.num_lines * sequence_length\n",
    "        ]\n",
    "\n",
    "        full_x_data = full_data.copy()\n",
    "        full_y_data = full_data.copy()\n",
    "        full_y_data[:-1] = full_data[1:]\n",
    "        full_y_data[-1] = full_data[0]\n",
    "\n",
    "        self.x_lines = full_x_data.reshape(self.num_lines, sequence_length)\n",
    "        self.y_lines = full_y_data.reshape(self.num_lines, sequence_length)\n",
    "    def __getitem__(self, index):\n",
    "        x = torch.LongTensor(self.x_lines[index])\n",
    "        y = torch.LongTensor(self.y_lines[index])\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_lines\n",
    "\n",
    "\n",
    "def load_data(input_path, sequence_length, batch_size):\n",
    "    \"\"\" Load data and get Dataset and Dataloader \"\"\"\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = f.read()\n",
    "\n",
    "    dataset = CharRNNDataset(text=data, sequence_length=sequence_length)\n",
    "    vocab_size = dataset.vocab_size\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=True,\n",
    "        num_workers=1)\n",
    "    return vocab_size, dataset, dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repackage_hidden(h):\n",
    "    \"\"\"Repackages a hidden state as a new variable. The stops gradients\n",
    "    from flowing back further.\"\"\"\n",
    "    if type(h) == Variable:\n",
    "        return Variable(h.data)\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)\n",
    "    \n",
    "def new_hidden(model, batch_size):\n",
    "    \"\"\"Get a new hidden variable\"\"\"\n",
    "    return repackage_hidden(model.init_hidden(batch_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNNModel(nn.Module):\n",
    "    def __init__(self, num_layers, rnn_size, vocab_size):\n",
    "        super(CharRNNModel, self).__init__()\n",
    "\n",
    "        # Parameters\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn_size = rnn_size\n",
    "        self.vocab_size = vocab_size\n",
    "        # Layers (containing weights)\n",
    "        self.embedding = nn.Embedding(\n",
    "            self.vocab_size, self.rnn_size,\n",
    "        )\n",
    "\n",
    "\n",
    "        self.rnn = nn.RNN(\n",
    "            self.rnn_size,\n",
    "            self.rnn_size,\n",
    "            self.num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc1 = nn.Linear(self.rnn_size, self.vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Inputs come in with dimensions (from data loaders):\n",
    "            [batch_size, seq_length]\n",
    "        \"\"\"\n",
    "\n",
    "        # Embed\n",
    "        embedded = self.embedding(x)\n",
    "    \n",
    "        # Push through RNN\n",
    "        lstm_out, hidden = self.rnn(embedded, hidden)\n",
    "\n",
    "        # Apply Linear layer\n",
    "        output = self.fc1(lstm_out)\n",
    "\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"Initialize hidden weights\"\"\"\n",
    "        weight = next(self.parameters()).data\n",
    "        h = Variable(\n",
    "            weight.new(self.num_layers, batch_size, self.rnn_size).zero_()\n",
    "        )\n",
    "        return h\n",
    "    \n",
    "def train(x, y, model, criterion, optimizer, h):\n",
    "    model.zero_grad()\n",
    "    model.train()\n",
    "    output, h = model(x, h)\n",
    "    loss = criterion(output.contiguous().view(-1, vocab_size), y.view(-1))\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
    "    optimizer.step()\n",
    "    return loss.data.cpu().numpy()[0], h\n",
    "\n",
    "def evaluate(model, h, seed, sequence_length):\n",
    "    model.eval()\n",
    "    seed_text = np.array(list(map(dataset.char_to_int_map.get, seed))).reshape(1, -1)\n",
    "    history = seed_text\n",
    "    for i in range(5):\n",
    "        predicted_input = history[:, -sequence_length:]\n",
    "        predicted_input_var = Variable(torch.LongTensor(predicted_input))\n",
    "        output, h = model(predicted_input_var, h)\n",
    "        history = np.hstack((history,np.argmax(torch.exp(output).data.numpy()[0], 1).reshape(1,-1)))\n",
    "    return history\n",
    "\n",
    "def sample(model, size=100, seed='The', top_k=None, cuda=False):\n",
    "    \"\"\" Sample characters from the model.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    seed_text = np.array(list(map(dataset.char_to_int_map.get, seed))).reshape(1, -1)\n",
    "    list_ = seed_text.reshape(-1)\n",
    "    h = model.init_hidden(1)\n",
    "    char, h = model(Variable(torch.LongTensor(seed_text)), h)\n",
    "    new_char = torch.max(F.softmax(char, dim=2)[:,2,:],1)[1]\n",
    "    list_ = seed_text.tolist()[0]\n",
    "    for i in range(100):\n",
    "        list_.append(new_char.data.view(1)[0])\n",
    "        new_char, h = model(new_char.view(1,1), h)\n",
    "        new_char = torch.max(F.softmax(new_char, dim=2),2)[1]\n",
    "    return (''.join(list(map(dataset.int_to_char_map.get, list_))))\n",
    "#     chars.append(char)\n",
    "\n",
    "#     for ii in range(size):\n",
    "#         char, h = model.predict(chars[-1], h, cuda=cuda, top_k=top_k)\n",
    "#         chars.append(char)\n",
    "\n",
    "#     return ''.join(chars)\n",
    "\n",
    "\n",
    "def sample(model, size=100, prime='The', top_k=None, cuda=False):\n",
    "    \"\"\" Sample characters from the model.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    chars = []\n",
    "    h = model.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        seed_text = Variable(torch.LongTensor(np.array(list(map(dataset.char_to_int_map.get, ch))).reshape(1, -1)))\n",
    "        h = Variable(h.data)\n",
    "        output, h = model(seed_text, h)\n",
    "        p = F.softmax(output, dim = 2).data\n",
    "        top_ch = np.arange(dataset.vocab_size)\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        chars.append(char)\n",
    "    for ii in range(size):\n",
    "        seed_text = Variable(torch.LongTensor(np.array(chars[-1]).reshape(1, -1)))\n",
    "        h = Variable(h.data)\n",
    "        output, h = model(seed_text, h)\n",
    "        p = F.softmax(output, dim = 2).data\n",
    "        top_ch = np.arange(dataset.vocab_size)\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        chars.append(char)\n",
    "    return (''.join(list(map(dataset.int_to_char_map.get, chars))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he greary, a at\\nposery of here\\npicthe carm Barching to\\nknow he had.\\n\\nNatasha had rusope.\\n\\nNater peaher '"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full data 3196213\n",
      "Num lines 127848\n",
      "Num lines 3196200\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "hidden_units = 100\n",
    "batch_size = 14\n",
    "vocab_size, dataset, dataloader = load_data(\"warpeace_input.txt\", sequence_length=25, batch_size=batch_size)\n",
    "model = CharRNNModel(num_layers=1, rnn_size=hidden_units, vocab_size=vocab_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1e-3,\n",
    ")\n",
    "losses = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1000 1.5897549 han the spcher of histolashed in\n",
      "less of all with assock of gry to half he said his latted andy\n",
      "seched.\n",
      "0 2000 1.6549367 he came,\" shistov pust on that intrancarm Boate, advant how gone up yet is\n",
      "be kison bles we fleare digh\n",
      "0 3000 1.6002694 oa dooreaty around his guante lift you.\n",
      "\n",
      "One warmatioul, but his lont to the gresperili, it. He has bee\n",
      "0 4000 1.6302135 he nearing on themseld have rightly Prince Vas that\n",
      "impin\n",
      "\" the old being thim. Having at acros hegs? D\n",
      "0 5000 1.7124991 ha oppresstry the\n",
      "woorwd. He e\"on.\n",
      "As who had not sawe trew that I\n",
      "mountrev in Raced.\n",
      "\n",
      "Dn ondles, but t\n",
      "0 6000 1.6285071 ha read it's servemested at the cart chieflomperiesting were sheace, the decisived as evenervisandy wer\n",
      "0 7000 1.7972686 ha prepitatiate. And in.\"\n",
      "\n",
      "Is a!\" said at a but can admill.\n",
      "\n",
      "If been onen drack obbed pleason... Thy. B\n",
      "0 8000 1.686118 ha expleade yam sudden, he campa at Brong which have cad fries' Now, awasont of thind\n",
      "sethe doing its m\n",
      "0 9000 1.7392954 he legan\n",
      "you sursenth. He would now a go very alsoasing to him, and deer I'll if as who see! \n",
      "Petya tha\n",
      "1 1000 1.5856512 ha officer oppread notely it. So, and spol and\n",
      "find tone tby only down with a\n",
      "\n",
      "Becontendes, all it say \n",
      "1 2000 1.6992614 oen the reached his panisoles attens what.\" weaki I wance. All returned?\" And they, they thought the lo\n",
      "1 3000 1.8835622 ha Goling decitese the man in finnings, in whom\n",
      "bacrive of\n",
      "she would heries\n",
      "Andrew's empperaction for a\n",
      "1 4000 1.635631 hare are\n",
      "the burnoto have, Pipud, who know, though seruact repay sobout on a called the bacted more wit\n",
      "1 5000 1.8936441 hi glankoond the oltf enter--harse. Fodation reposicady,\n",
      "just behillown. \"Iskionski restovas with, e le\n",
      "1 6000 1.6519557 ha shave to Moscow oplexlentain but in him face in the hand soldine\n",
      "and the one amaduce deference ow me\n",
      "1 7000 1.5079721 ho she old\n",
      "service the bit. The officer spoke and\n",
      "and I\n",
      "she rages\n",
      "front, te Napoleongly, tone! I? I've \n",
      "1 8000 1.5705068 ham prauginewe her wornty his alte for faceess-conse inverances. And Look, most deachiend roving to the\n",
      "1 9000 1.7277323 oi vilent demorors toge the grow, the bole\n",
      "French very lay the finsk a joing anistaniness. \"It e casons\n",
      "2 1000 1.6617644 ho sound tuming to the pr to her (was cloud, where be clood, Alrel which you'll's by off the\n",
      "worls.\n",
      "\n",
      "Fr\n",
      "2 2000 1.6302984 hi next\n",
      "bosk and recegare histonthey whisovs sternicer amid, \"5!\"Wat\n",
      "carricisity?\"\n",
      "ste an him Moscow ot\n",
      "2 3000 1.5631738 hair what Princess face. \"And seements:'\n",
      "\n",
      "\"I'll there of the own seemed on the armselfect a first a lov\n",
      "2 4000 1.6443591 hiy are don't him table, and\n",
      "make out as in hat curged to dreans, can and turning tree tice\n",
      "in-chigherl\n",
      "2 5000 1.4852555 ha man Natasha rost hall armed ll you tooked\n",
      "a usuched at all on began the\n",
      "will, Yes, latyany dad the m\n",
      "2 6000 1.5340238 he rimbalee. Tentimut anot and\n",
      "generars, and the door, expectening mart and inscial old\n",
      "hnequely few th\n",
      "2 7000 1.5390915 hey some was brother that all starce at hopeled, and helous to she this d been where talk) flancis foni\n",
      "2 8000 1.6336458 hay), or\n",
      "when yaush orders of the Majested if he camp, the goorzed\n",
      "him,\n",
      "aimed you servicamely over, rid\n",
      "2 9000 1.7816991 han his\n",
      "rooked and then with the French you the youngen time and his shove\n",
      "by the iced done sky lable, \n",
      "3 1000 1.7339771 ha sable, and loous right of the\n",
      "mognibity.\"\n",
      "\n",
      "\"Well. On the power that suppeain the\n",
      "Frenting awantion a\n",
      "3 2000 1.7139549 hi pood ind princess of her lear! Lear even of a eld\n",
      "princess with vesse. Rostov site of thes feached h\n",
      "3 3000 1.6514859 ha face say grief fell be are haptronchite ran't infor. But they would nginices; Alam-sied founder rke \n",
      "3 4000 1.7069746 he God. Theiscust. One lyo.\n",
      "\n",
      "\"And to despenized to\n",
      "Pierre an mode by\n",
      "gailoncing to felt\n",
      "be the losoughs\n",
      "3 5000 1.9746809 heir ghthis my of siding on the country.\n",
      "The of spoke of leaving is a not his\n",
      "das Denaited, but view he\n",
      "3 6000 1.6346748 ha night with by urting a leon his better ap oppo farther's\n",
      "readdre to conforted to the besses could an\n",
      "3 7000 1.8099964 here' men should be clogned thing\n",
      "prepossible of of the lady's rrone Natasha's XIV her\n",
      "all still my cit\n",
      "3 8000 1.6444486 hi Leg, which the feeling which which Captais, but the wor. Then and expered and he\n",
      "elane served he\n",
      "rid\n",
      "3 9000 1.6550056 ha passion,\n",
      "oncee fors litts, as he lower thir ll the French. You till offic this woman she firs.\n",
      "He ha\n",
      "4 1000 1.6776801 hyn invinoce agoments she eyes,\n",
      "stopwing of the spanions and happeneritspossilitord, quite ic thenim, \"\n",
      "4 2000 1.7294025 he seemanemence grew end had\n",
      "good and ask that\n",
      "sick?\" he bately at her feelned.\n",
      "One see suddenly in abs\n",
      "4 3000 1.6999602 ha persuch they to\n",
      "self. \"If men, awn his eyes. Eh,\n",
      "a kemy--firche a clood bass fingers of mo in phing \n",
      "4 4000 1.5993098 here all.\n",
      "\n",
      "\"Verrieir to reasons who contrid, with the Counded to moodently would not\n",
      "hon myselves!\" he \n",
      "4 5000 1.6314765 ha commanded her splec.\n",
      "\n",
      "\"I.\n",
      "\n",
      "\"Napoleon a got Piing, Zhater.\n",
      "\n",
      "\"Yer also up of evener to be looked Bluss\n",
      "4 6000 1.6924427 hi please smile on hot around on the pouzat yourself drubbrate felt in The place of the\n",
      "man you\n",
      "married\n",
      "4 7000 1.5669726 han they dince happens want liem the Russia propected the generained the cablace-doctor-- the splessain\n",
      "4 8000 1.7330022 he gon were id nos one came for and to addres it I mays.\n",
      "Roythe Kary the vaniforsed accountried buidle'\n",
      "4 9000 1.6658603 hae regreater\n",
      "stopped and pushu\n",
      "fathinch would Shaving. Napolegs,\" whisomse as he knew!\" He was.\n",
      "\n",
      "\"I sa\n",
      "5 1000 1.6014886 hi feome beip bother of the man was in the vaup, and and looked and and forward with the fing to all th\n",
      "5 2000 1.738805 he canded and lost of all curatiance and any dymad cold her det of the voute\"\n",
      "\n",
      "It had lear\n",
      "is jarest--I\n",
      "5 3000 1.6942399 ha sary of 1813 Natasha room forwovary stooptions, this are someov, the Pierre clould\n",
      "all table again. \n",
      "5 4000 1.7699269 he Emprow exchien disposite my told lasses feelega was ereicia\n",
      "was not all no all thought, the srugge w\n",
      "5 5000 1.7306793 ha kis...\n",
      "\n",
      "Denisom and someal the s\" Boneht.\n",
      "\n",
      "Het be the hourning\n",
      "this,\" olothk. Eallanged this good to\n",
      "5 6000 1.7448244 hi French to because.\n",
      "\n",
      "And I\n",
      "have threatran ag to be will nsing may to Frenchmoved exteghter, and the F\n",
      "5 7000 1.5987918 ha old minoth the weptung Nicholas frse king from moxt the are I can which the Emperor's up\n",
      "and more at\n",
      "5 8000 1.6399177 ha dirent nlow to Momen as for ome. That itiles and the fice or drack for going of the power it ond har\n",
      "5 9000 1.5740135 hire dardssmout pacing the surcome step bed mistagen blue, whom it fewand early already a whalments say\n",
      "6 1000 1.6607922 here he has they is someince dis way from when God, Godngryisorted to have look!\" grappered were frighb\n",
      "6 2000 1.5827619 hi grince Andrew.\n",
      "\n",
      "That He wrogity. Daving out incallet to old not having my so breadly and the\n",
      "Fossily\n",
      "6 3000 1.656207 ha cormon?\"\n",
      "\n",
      "The neche have talk what the count supling the was companion Natasha will given's moring t\n",
      "6 4000 1.6550862 han\n",
      "instaction of felt know\n",
      "sat to if the ustion, Youth than he eper same rhov agay upon..\n",
      "Allang to pr\n",
      "6 5000 1.7073334 hi\n",
      "that who ringlends an Russian--the vern freseriration, were not appaking his casuned his man yourl, \n",
      "6 6000 1.6630254 he face acclanced Vasushene. Oh; Dulashapped had took looker, leeposting. July eption and victo was Br \n",
      "6 7000 1.6612425 here which shre where don attence's\n",
      "med a state soizled about armstable, face\n",
      "ed and ending for the\n",
      "mi?\n",
      "6 8000 1.5384517 he in the\n",
      "no life. \"Baggant topsion\n",
      "e must people who were, he leappied\n",
      "tip, milled my lost quite ed of\n",
      "6 9000 1.7713505 he fact, and all; that nott stomens, but unimine an ours at\n",
      "my glanced to be weemom asked to\n",
      "mind, expr\n",
      "7 1000 1.7723488 hese and s\n",
      "the place thand anis blue gover and\n",
      "escenfacted same from it waide what you ready.\n",
      "\n",
      "\"Rosop. \n",
      "7 2000 1.6808953 he A tore of is obend and threed him. \"Mary and stirrasanky weveraby eree with himself loors in the per\n",
      "7 3000 1.7069224 hi thousandian her man?\" and\n",
      "Nichosov.\n",
      "\n",
      "Frosse. But his wall asked to run\n",
      "\"Cend the,\" said had footmen,\n",
      "7 4000 1.6242609 he husters exceag to mearal and\n",
      "him advicy a louzabling\n",
      "received hoswits unrout away was garding in him\n",
      "7 5000 1.7323734 hi station.\n",
      "\n",
      "\"If he compondissed that the looked not man the coat-killies been child--Pierre is only wi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 6000 1.6632835 he feelies swith in voice esplatching of the come all and see them\n",
      "sever up,\" he stayarch crow, y--mood\n",
      "7 7000 1.6881416 hire would father is buglof in the lation his\n",
      "puring,\" said Rostov: He had bud. \"After read ve, flancl.\n",
      "7 8000 1.769745 hen feness, never. Deatisfined and fine was\n",
      "youngen there, and had ng it the sile othin and full\n",
      "till t\n",
      "7 9000 1.608826 hi possuto an more Plated of a soce the French\n",
      "he scopped loved of his. Pierre or\n",
      "Natasha he\n",
      "is five on\n",
      "8 1000 1.712978 here pring to his lod\n",
      "clong at down susmench place mordendly beaute\n",
      "didesoped an alway,\n",
      "nuture aiming t\n",
      "8 2000 1.6873978 hen\n",
      "the taken\n",
      "on the girl understangen, and strery\" of can carcadehereigs, third with my es, and adder,\n",
      "8 3000 1.696944 hare and one\n",
      "murs docist difficult, to\n",
      "ran-way...\" 'Madry you showing Bremaid slarch and surple\n",
      "decking\n",
      "8 4000 1.8148706 he mosoce the\n",
      "valeadvis' band! With that facton fro's went down he all\n",
      "no remy wish it's he while the m\n",
      "8 5000 1.6543353 ha seeloson's flumcting plalmed the question. But the Powers it was nowerst placed to the anied \"ther w\n",
      "8 6000 1.51116 hi midationy..\"\n",
      "\n",
      "\"Forginow!\" she wanted the Pierre since ordered his knew arould havingry men, and off \n",
      "8 7000 1.6740464 he pilease noldy\n",
      "wermiss who was roand a gover on him of him. The Rostov again chaminut all not met wit\n",
      "8 8000 1.8072377 hen'w looked\n",
      "no\n",
      "something expectible to\n",
      "life contrasithe d instanter also trount alsoparunt in the riqu\n",
      "8 9000 1.695042 hin on\n",
      "When's\n",
      "come formest riding. He sound to\n",
      "a seemes in Pierre,\n",
      "and seemed, interet. \"He als not the\n",
      "9 1000 1.6256412 he gurer toat and commons and\n",
      "Kid the peace, they was essbbated.\n",
      "\n",
      "The so as as appeared Ot. He was. She\n",
      "9 2000 1.754034 he cappiment frouts seenll.\n",
      "\n",
      "A kee and lammendeffin't side of espla, as\n",
      "as o\n",
      "deinow of the vesturieumar\n",
      "9 3000 1.7454494 he underverod on histothee. Serfily up than her self-controomact Den, told her looks, a face.\n",
      "\n",
      "\"Fry. Wh\n",
      "9 4000 1.7359065 ha soldier. Dexigia rated arped left and drurthe I knew he cordoady, what 55 \"\n",
      "\n",
      "* \"Wighting seemed as, \n",
      "9 5000 1.7778696 hen they was the matters!\"\n",
      "he masnonely,\" said Sonyan'qune her his retsking,\n",
      "moverach in\n",
      "nos, stood coa\n",
      "9 6000 1.5059075 hen by addeirtly conversatoreful as\n",
      "facts, platess.\n",
      "\n",
      "\"The owny borassced rnow freing sigheopiting for r\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    h = model.init_hidden(14)\n",
    "    counter = 0\n",
    "    batch_loss = 0\n",
    "    for x, y in dataloader:\n",
    "        counter += 1\n",
    "        x_var, y_var = Variable(x), Variable(y)\n",
    "        this_batch_size = x_var.size()[0]\n",
    "        h = Variable(h.data)\n",
    "        loss_val, h = train(x_var,y_var, model, criterion, optimizer, \\\n",
    "                           h=h)\n",
    "        batch_loss += loss_val/this_batch_size\n",
    "        \n",
    "        if counter % 1000 == 0:\n",
    "            prediction = sample(model)\n",
    "#             prediction = evaluate(model, new_hidden(model, batch_size=1), 'The ', sequence_length)\n",
    "            print(epoch, counter, loss_val, prediction)\n",
    "    losses.append(batch_loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAEKCAYAAADq59mMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdf5a1d7588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(losses)), losses, 'b', label='smooth loss')\n",
    "plt.xlabel('time in thousands of iterations')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray([45,59,56,59,56, 1, 59]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval(model, model.init_hidden(1), 'The')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127848\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
