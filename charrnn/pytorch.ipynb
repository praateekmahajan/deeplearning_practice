{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/praateek/miniconda3/envs/ml/lib/python3.6/site-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['sample']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F \n",
    "import torch.utils.data\n",
    "import  torch.optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNNDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, text, sequence_length):\n",
    "        self.char_ls = sorted(list(set(text)))\n",
    "        self.vocab_size = len(self.char_ls)\n",
    "        self.char_to_int_map = dict(zip(self.char_ls, range(len(self.char_ls))))\n",
    "        self.int_to_char_map = dict(zip(range(len(self.char_ls)), self.char_ls))\n",
    "        full_data = np.array(list(map(self.char_to_int_map.get, text)))\n",
    "        # Truncate slightly so we have a num_chars = num_lines * seq_length\n",
    "        print(\"Full data\", len(full_data))\n",
    "\n",
    "        self.num_lines = int(len(full_data) / sequence_length)\n",
    "        print(\"Num lines\", self.num_lines)\n",
    "        print(\"Num lines\", self.num_lines * sequence_length)\n",
    "\n",
    "        full_data = full_data[\n",
    "            :self.num_lines * sequence_length\n",
    "        ]\n",
    "\n",
    "        full_x_data = full_data.copy()\n",
    "        full_y_data = full_data.copy()\n",
    "        full_y_data[:-1] = full_data[1:]\n",
    "        full_y_data[-1] = full_data[0]\n",
    "\n",
    "        self.x_lines = full_x_data.reshape(self.num_lines, sequence_length)\n",
    "        self.y_lines = full_y_data.reshape(self.num_lines, sequence_length)\n",
    "    def __getitem__(self, index):\n",
    "        x = torch.LongTensor(self.x_lines[index])\n",
    "        y = torch.LongTensor(self.y_lines[index])\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_lines\n",
    "\n",
    "\n",
    "def load_data(input_path, sequence_length, batch_size):\n",
    "    \"\"\" Load data and get Dataset and Dataloader \"\"\"\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = f.read()\n",
    "\n",
    "    dataset = CharRNNDataset(text=data, sequence_length=sequence_length)\n",
    "    vocab_size = dataset.vocab_size\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=True,\n",
    "        num_workers=1)\n",
    "    return vocab_size, dataset, dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repackage_hidden(h):\n",
    "    \"\"\"Repackages a hidden state as a new variable. The stops gradients\n",
    "    from flowing back further.\"\"\"\n",
    "    if type(h) == Variable:\n",
    "        return Variable(h.data)\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)\n",
    "    \n",
    "def new_hidden(model, batch_size):\n",
    "    \"\"\"Get a new hidden variable\"\"\"\n",
    "    return repackage_hidden(model.init_hidden(batch_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNNModel(nn.Module):\n",
    "    def __init__(self, num_layers, rnn_size, vocab_size):\n",
    "        super(CharRNNModel, self).__init__()\n",
    "\n",
    "        # Parameters\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn_size = rnn_size\n",
    "        self.vocab_size = vocab_size\n",
    "        # Layers (containing weights)\n",
    "        self.embedding = nn.Embedding(\n",
    "            self.vocab_size, self.rnn_size,\n",
    "        )\n",
    "\n",
    "\n",
    "        self.rnn = nn.RNN(\n",
    "            self.rnn_size,\n",
    "            self.rnn_size,\n",
    "            self.num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc1 = nn.Linear(self.rnn_size, self.vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Inputs come in with dimensions (from data loaders):\n",
    "            [batch_size, seq_length]\n",
    "        \"\"\"\n",
    "\n",
    "        # Embed\n",
    "        embedded = self.embedding(x)\n",
    "    \n",
    "        # Push through RNN\n",
    "        lstm_out, hidden = self.rnn(embedded, hidden)\n",
    "\n",
    "        # Apply Linear layer\n",
    "        output = self.fc1(lstm_out)\n",
    "\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"Initialize hidden weights\"\"\"\n",
    "        weight = next(self.parameters()).data\n",
    "        h = Variable(\n",
    "            weight.new(self.num_layers, batch_size, self.rnn_size).zero_()\n",
    "        )\n",
    "        return h\n",
    "    \n",
    "def train(x, y, model, criterion, optimizer, h):\n",
    "    model.zero_grad()\n",
    "    model.train()\n",
    "    output, h = model(x, h)\n",
    "    loss = criterion(output.contiguous().view(-1, vocab_size), y.view(-1))\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
    "    optimizer.step()\n",
    "    return loss.data.cpu().numpy()[0], h\n",
    "\n",
    "def evaluate(model, h, seed, sequence_length):\n",
    "    model.eval()\n",
    "    seed_text = np.array(list(map(dataset.char_to_int_map.get, seed))).reshape(1, -1)\n",
    "    history = seed_text\n",
    "    for i in range(5):\n",
    "        predicted_input = history[:, -sequence_length:]\n",
    "        predicted_input_var = Variable(torch.LongTensor(predicted_input))\n",
    "        output, h = model(predicted_input_var, h)\n",
    "        history = np.hstack((history,np.argmax(torch.exp(output).data.numpy()[0], 1).reshape(1,-1)))\n",
    "    return history\n",
    "\n",
    "def sample(model, size=100, seed='The', top_k=None, cuda=False):\n",
    "    \"\"\" Sample characters from the model.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    seed_text = np.array(list(map(dataset.char_to_int_map.get, seed))).reshape(1, -1)\n",
    "    list_ = seed_text.reshape(-1)\n",
    "    h = model.init_hidden(1)\n",
    "    char, h = model(Variable(torch.LongTensor(seed_text)), h)\n",
    "    new_char = torch.max(F.softmax(char, dim=2)[:,2,:],1)[1]\n",
    "    list_ = seed_text.tolist()[0]\n",
    "    for i in range(100):\n",
    "        list_.append(new_char.data.view(1)[0])\n",
    "        new_char, h = model(new_char.view(1,1), h)\n",
    "        new_char = torch.max(F.softmax(new_char, dim=2),2)[1]\n",
    "    return (''.join(list(map(dataset.int_to_char_map.get, list_))))\n",
    "#     chars.append(char)\n",
    "\n",
    "#     for ii in range(size):\n",
    "#         char, h = model.predict(chars[-1], h, cuda=cuda, top_k=top_k)\n",
    "#         chars.append(char)\n",
    "\n",
    "#     return ''.join(chars)\n",
    "\n",
    "\n",
    "def sample(model, size=100, prime='The', top_k=None, cuda=False):\n",
    "    \"\"\" Sample characters from the model.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    chars = []\n",
    "    h = model.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        seed_text = Variable(torch.LongTensor(np.array(list(map(dataset.char_to_int_map.get, ch))).reshape(1, -1)))\n",
    "        h = Variable(h.data)\n",
    "        output, h = model(seed_text, h)\n",
    "        p = F.softmax(output, dim = 2).data\n",
    "        top_ch = np.arange(dataset.vocab_size)\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        chars.append(char)\n",
    "    for ii in range(size):\n",
    "        seed_text = Variable(torch.LongTensor(np.array(chars[-1]).reshape(1, -1)))\n",
    "        h = Variable(h.data)\n",
    "        output, h = model(seed_text, h)\n",
    "        p = F.softmax(output, dim = 2).data\n",
    "        top_ch = np.arange(dataset.vocab_size)\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        chars.append(char)\n",
    "    return (''.join(list(map(dataset.int_to_char_map.get, chars))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he greary, a at\\nposery of here\\npicthe carm Barching to\\nknow he had.\\n\\nNatasha had rusope.\\n\\nNater peaher '"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full data 3196213\n",
      "Num lines 127848\n",
      "Num lines 3196200\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "hidden_units = 100\n",
    "batch_size = 14\n",
    "vocab_size, dataset, dataloader = load_data(\"warpeace_input.txt\", sequence_length=25, batch_size=batch_size)\n",
    "model = CharRNNModel(num_layers=1, rnn_size=hidden_units, vocab_size=vocab_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1e-3,\n",
    ")\n",
    "losses = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1000 1.9434893 he guard e got refurng he\n",
      "have the oxcest aperf andernusee fading mwnat, a wald, to of tf the wa! Frono\n",
      "0 2000 1.9535059 ho wy shom let he sond but shays bush hes, a Domeglening vodyen sidly of one An though these othrel how\n",
      "0 3000 1.9486865 hr restov of inted sontond reciog told your remed only tonde knook\n",
      "the vaplainstly hor udened. Befraile\n",
      "0 4000 1.8340533 hen On reapan thin he sontuinces, way futurt he harped Boow gl mude, ne were of with asskiberts Pierde \n",
      "0 5000 1.887704 here about nown Ifngerels severied the afd the yet with to along\n",
      "to we te arrisu,\" Andravis the house: \n",
      "0 6000 1.7752715 hea shown't\n",
      "chers and battant to bevineral! \"I'll,\"\n",
      "he duffad and Rostov ir uninvicar acked ank denit i\n",
      "0 7000 1.689134 hey Andytleyond mether hoult Beaces of comploy till looked on the Russiasn young out in Ruplestan that \n",
      "0 8000 1.7987539 hen migned in the like the zurred his plank of lookederats. He facent's I'm officured.\n",
      "\n",
      "Princle the lit\n",
      "0 9000 1.7548233 hi sumbrignlightragel at gothe army of: kill you grultice, so him,\n",
      "handed it oProted.\n",
      "\n",
      "\"The offorardue,\n",
      "1 1000 1.870067 he Ewing ressevee of prove of they was is of stand--infuill has imble counttinu of her entered womprove\n",
      "1 2000 1.8248374 re had\n",
      "been sigh caves chaod\n",
      "eved ind'muructiltonremain, had on taver being for all of the orld\n",
      "lear, h\n",
      "1 3000 1.7434775 he neceples and, the railed Verier....\n",
      "\n",
      "And because, to the murse fur made agn formern herd when said h\n",
      "1 4000 1.8343785 he Bostervice sign. Belitinely or te sinking lyaging move ende the manyoney's winguar and is Nicholas a\n",
      "1 5000 1.6022274 hi\n",
      "villicand were not uwittrles, conel of whitho continuetered. This were beed and the one fe but. Tupl\n",
      "1 6000 1.7007856 here callogndersthier and sollincess croops commanding, on the placeing Emperor and\n",
      "somity-rosen seever\n",
      "1 7000 1.6587458 he shoums'' would dosasant and fold frunctions the be you, and diresuls. Pierre's much reident began fa\n",
      "1 8000 1.7633373 hen brother, are had our able push ruppauttice, drothed love in that's durge his\n",
      "nother even with go li\n",
      "1 9000 1.6327658 hyy with. Natashaaking. You won\n",
      "Napoleay was how alar, wish to gre, snoperyon\n",
      "self carce ad some contar\n",
      "2 1000 1.71217 he divavivedinnembeit. On a from villich grow, a quithey how\n",
      "ach reid any to him.\n",
      "And his look?\" Race t\n",
      "2 2000 1.7133126 hen givently at?\" she remem firenced, soldier the soldier if the sutch crit was not two trdelly strespe\n",
      "2 3000 1.6643971 hore were all; his Agforn to the very freaze tind\n",
      "present, and folled in away twing lowentlya and, old\n",
      "\n",
      "2 4000 1.5831394 hi for at ait buled left Anclustiaselders'\n",
      "\n",
      "\"To to her sunder of desilive; and evently\n",
      "togen's a beforc\n",
      "2 5000 1.791996 here appeaftening own. She wor and d her\n",
      "and Bear Berghle talk the basish to che bessing longer exal ex\n",
      "2 6000 1.6638749 hi silent of sole. \"Yes, and power. Effected to forget, umy with her conspructed wearing out him of a z\n",
      "2 7000 1.752437 he reasants wichen the pole dighthole were noth for attence, fram, and inthathing people been only, and\n",
      "2 8000 1.7779082 hi regaibety from\n",
      "cble he heise, and be\n",
      "stuarmer a! Napoleony in thind, plompan manth apressions in Son\n",
      "2 9000 1.5132624 he Ecrown role had he loving.\n",
      "Lavoutho was ent, sty two in-Mindided that anothing, wish the and solest;\n",
      "3 1000 1.7155268 he had a busking, that) some but had om or guested fellow! I've did mist doderons occurgearded by, sati\n",
      "3 2000 1.8338996 he sominds be armight thad that this and began and up at Emunylough there she imma's\n",
      "life His a seresti\n",
      "3 3000 1.5340523 he Schers, and deused my again turk from\n",
      "itselperieble in adjuwitation to Denety beeth he was to\n",
      "didly\n",
      "\n",
      "3 4000 1.7816457 hed everyone and preun them.\n",
      "Kusked on one\n",
      "head s gone kin this coge and behind. \"She was starits Minqu\n",
      "3 5000 1.6566584 hare\n",
      "len of could aroup rilled out the rather rein arcompy,\" awors gaesivers), with the kneatt of the f\n",
      "3 6000 1.8226899 han corp, and worlles by so as her to eyes, wours from the counthere of got ry mmost threw you. On a th\n",
      "3 7000 1.6015964 he\n",
      "Fron's counth featarfit?\" counting she was ask felt generaps. Fronimms.\n",
      "\n",
      "A tain were from stointov o\n",
      "3 8000 1.6112994 ha?... one clussed. \"I,\" and destrovoul it, the plaralsain\n",
      "or. \"To the offety\n",
      "began help towas they. In\n",
      "3 9000 1.8406013 ho Sank had her said Vute\n",
      "convinds frighter about raint for feel misen not her\n",
      "did not her played first\n",
      "4 1000 1.6273928 heases.\"\n",
      "\n",
      "\"But I ruised Russian, took had since\n",
      "relizer to Madked about.\n",
      "\n",
      "Pierre's pusome usestorianing\n",
      "4 2000 1.8098494 hente out sppure a frid not the last that\n",
      "they down would palven and kersa.\n",
      "\n",
      "On his enexcestaiggry pide\n",
      "4 3000 1.6730374 hey vo buse on.\"\n",
      "\n",
      "\"Kutanthip\n",
      "but he says\n",
      "proverovs's take fla agtenemans and woiling throughu'ly cares \n",
      "4 4000 1.6843835 here estatal flooparto God that he went and devoidg hard not not the Emperore-for a stood any said whet\n",
      "4 5000 1.640219 ha yonerty and was a momil noties.\n",
      "\n",
      "\"Cosacher, he say but as alang and she entimaing her what feglan, a\n",
      "4 6000 1.7373737 ha time, ir drawsation receyed him Mo dike he im, and and the engs thought tran dectalflary a\n",
      "lords. Yo\n",
      "4 7000 1.6333449 hor of Sleng.\n",
      "\n",
      "\"Well, itse; hershed moment or in asked this theisich I aman it had sored.\n",
      "\n",
      "\"You and a c\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    h = model.init_hidden(14)\n",
    "    counter = 0\n",
    "    batch_loss = 0\n",
    "    for x, y in dataloader:\n",
    "        counter += 1\n",
    "        x_var, y_var = Variable(x), Variable(y)\n",
    "        this_batch_size = x_var.size()[0]\n",
    "        h = Variable(h.data)\n",
    "        loss_val, h = train(x_var,y_var, model, criterion, optimizer, \\\n",
    "                           h=h)\n",
    "        batch_loss += loss_val/this_batch_size\n",
    "        \n",
    "        if counter % 1000 == 0:\n",
    "            prediction = sample(model)\n",
    "#             prediction = evaluate(model, new_hidden(model, batch_size=1), 'The ', sequence_length)\n",
    "            print(epoch, counter, loss_val, prediction)\n",
    "    losses.append(batch_loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQIiISFb/HwOhaqFYRRJiAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdf5d420fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(losses)), losses, 'b', label='smooth loss')\n",
    "plt.xlabel('time in thousands of iterations')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray([45,59,56,59,56, 1, 59]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval(model, model.init_hidden(1), 'The')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127848\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
