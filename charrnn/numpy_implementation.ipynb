{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 3196213 characters, 83 unique.\n"
     ]
    }
   ],
   "source": [
    "data = open('warpeace_input.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "class DataReader:\n",
    "    def __init__(self, path, seq_length):\n",
    "        self.fp = open(path, \"r\")\n",
    "        self.data = self.fp.read()\n",
    "        vocab = list(set(self.data))\n",
    "        self.char_to_ix = {ch:i for (i,ch) in enumerate(vocab)}\n",
    "        self.ix_to_char = {i:ch for (i,ch) in enumerate(vocab)}\n",
    "        self.data_size = len(self.data)\n",
    "        self.vocab_size = len(vocab)\n",
    "        self.pointer = 0\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def next_batch(self):\n",
    "        input_start = self.pointer\n",
    "        input_end = self.pointer + self.seq_length\n",
    "        inputs = [self.char_to_ix[ch] for ch in self.data[input_start:input_end]]\n",
    "        targets = [self.char_to_ix[ch] for ch in self.data[input_start+1:input_end+1]]\n",
    "        self.pointer += self.seq_length\n",
    "        \n",
    "        # reset pointer\n",
    "        if self.pointer + self.seq_length + 1 >= self.data_size:\n",
    "            input_start = self.pointer\n",
    "            input_end = self.data_size\n",
    "            inputs = [self.char_to_ix[ch] for ch in self.data[input_start:input_end]]\n",
    "            targets = [self.char_to_ix[ch] for ch in self.data[input_start+1:input_end+1]]\n",
    "            self.pointer = self.data_size\n",
    "        return inputs, targets\n",
    "\n",
    "    def just_started(self):\n",
    "        return self.pointer == 0\n",
    "\n",
    "    def epoch_end(self):\n",
    "        return self.pointer == self.data_size\n",
    "\n",
    "    \n",
    "    def close(self):\n",
    "        self.fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F:\n",
    "    def softmax(x):\n",
    "        \"\"\"Compute softmax values for each sets of scores in x.\n",
    "\n",
    "        Subtracts max(x) for numerical stability\n",
    "        Args:\n",
    "            x: an array of size n * 1.\n",
    "        Returns:\n",
    "            A probability distribution over the vector x\n",
    "        \"\"\"\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum()\n",
    "    def crossentropy(pred_prob, target):\n",
    "        return sum(-np.log(pred_prob[t][target[t],0]) for t in range(len(pred_prob.keys())))\n",
    "\n",
    "\n",
    "class RNN:\n",
    "    def __init__(self, num_hidden, num_input):\n",
    "        self.input_size = num_input\n",
    "        self.W_hx = np.random.randn(num_hidden, num_input)*0.01 # input to hidden of dimension hidden * input\n",
    "        self.W_hh = np.random.randn(num_hidden, num_hidden)*0.01 # hidden to hidden of dimension hidden * hidden\n",
    "        self.W_hy = np.random.randn(num_input, num_hidden)*0.01 # hidden to output of dimension input * hidden\n",
    "        \n",
    "        self.b_h = np.zeros((num_hidden, 1)) # hidden bias of dimension hidden * 1\n",
    "        self.b_y = np.zeros((num_input, 1)) # output bias of dimension input * 1\n",
    "\n",
    "        self.h = np.random.randn(num_hidden,1) * 0.01\n",
    "        \n",
    "        # memory vars for adagrad\n",
    "        self.mWxh = np.zeros_like(self.W_hx)\n",
    "        self.mWhh = np.zeros_like(self.W_hh)\n",
    "        self.mWhy = np.zeros_like(self.W_hy)\n",
    "        self.mbh = np.zeros_like(self.b_h)\n",
    "        self.mby = np.zeros_like(self.b_y)\n",
    "\n",
    "        \n",
    "    def step(self, x):\n",
    "        \"\"\"Compute a single step using hidden states for a given x.\n",
    "        Computes three dictionaries x_hat, h, p with keys = timestep\n",
    "        \n",
    "        \n",
    "        Mathematically it is h(t) = tanh((Wh * h(t-1)) + (Wx *x) + bias)\n",
    "                             y = Wy * h + bias\n",
    "        Args:\n",
    "            x: a vector of size n * 1.\n",
    "        Returns:\n",
    "            x_hat: a dictionary of one hot vectors of input for each time step\n",
    "            h: a dictionary of hidden states of each time step\n",
    "            p: Predicted probabilities of each timestep\n",
    "        \"\"\"\n",
    "        h, y_hat, x_hat, p = {}, {}, {}, {}\n",
    "        h[-1] = np.zeros(self.h.shape)\n",
    "\n",
    "        for t in range(len(x)):\n",
    "            x_hat[t] = np.zeros((self.input_size,1)) # Convert x into 1-hot-vector\n",
    "            x_hat[t][x[t]] = 1 # Set the index x[t] of 1-hot-vector x_hat[t] to 1\n",
    "            h[t] = np.tanh(np.dot(self.W_hh, h[t-1]) + np.dot(self.W_hx, x_hat[t]) + self.b_h)\n",
    "            y_hat[t] = np.dot(self.W_hy, h[t]) + self.b_y\n",
    "            p[t] = F.softmax(y_hat[t])\n",
    "        return x_hat, h, p\n",
    "    \n",
    "    \n",
    "    def backward(self, x, y, p, h, clip_val=5):\n",
    "        \"\"\"Backpropogates through a sequence\n",
    "\n",
    "        Args:\n",
    "            x: a dictionary of one hot vectors of input for each time step\n",
    "            y: vector containing real output values of dimension n*1\n",
    "            p: a dictionary of predicted probabilities\n",
    "            h: a dictionary of hidden states\n",
    "            clip_val: Clip values of gradient so it doesn't explode\n",
    "        Returns:\n",
    "            dW_hx, dW_hh, dW_hy, db_h, db_y: derivatives of weight vectors and bias\n",
    "        \"\"\"\n",
    "\n",
    "        dW_hx = np.zeros_like(self.W_hx)\n",
    "        dW_hh = np.zeros_like(self.W_hh)\n",
    "        dW_hy = np.zeros_like(self.W_hy)\n",
    "        db_h = np.zeros_like(self.b_h)\n",
    "        db_y = np.zeros_like(self.b_y)\n",
    "        dh_next = np.zeros_like(self.h)\n",
    "\n",
    "        for t in reversed(range(len(x))):\n",
    "            #backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "            dy = np.copy(p[t])\n",
    "            dy[y[t]] -= 1\n",
    "\n",
    "            #find updates for y\n",
    "            dW_hy += np.dot(dy, h[t].T)\n",
    "            db_y += dy\n",
    "\n",
    "            #backprop into h and through tanh nonlinearity\n",
    "            dh = np.dot(self.W_hy.T, dy) + dh_next\n",
    "            dh_raw = (1 - h[t]**2) * dh\n",
    "\n",
    "            #find updates for h\n",
    "            dW_hx += np.dot(dh_raw, x[t].T)\n",
    "            dW_hh += np.dot(dh_raw, h[t-1].T)\n",
    "            db_h += dh_raw\n",
    "\n",
    "            #save dh_next for subsequent iteration\n",
    "            dh_next = np.dot(self.W_hh.T, dh_raw)\n",
    "            \n",
    "        # clip to mitigate exploding gradients\n",
    "        for dparam in [dW_hx, dW_hh, dW_hy, db_h, db_y]:\n",
    "            np.clip(dparam, -clip_val, clip_val, out=dparam) \n",
    "        return dW_hx, dW_hh, dW_hy, db_h, db_y\n",
    " \n",
    "    \n",
    "    def sgd(self, dWxh, dWhh, dWhy, dbh, dby, learning_rate=1e-1):\n",
    "        \"\"\"Updates weights with vanilla gradient descent\n",
    "\n",
    "        Args:\n",
    "            dW_hx, dW_hh, dW_hy, db_h, db_y: derivatives of weight vectors and bias\n",
    "            learning_rate: Learning Rate for SGD\n",
    "        \"\"\"\n",
    "        for param, dparam in zip([self.W_hx, self.W_hh, self.W_hy, self.b_h, self.b_y],\n",
    "                                  [dWxh, dWhh, dWhy, dbh, dby]):\n",
    "            param += -learning_rate * dparam\n",
    "\n",
    "    def adagrad(self, dWxh, dWhh, dWhy, dbh, dby, learning_rate=1e-1):\n",
    "        \"\"\"Updates weights with adagrad\n",
    "\n",
    "        Args:\n",
    "            dW_hx, dW_hh, dW_hy, db_h, db_y: derivatives of weight vectors and bias\n",
    "            learning_rate: Learning Rate for Adagrad\n",
    "        \"\"\"\n",
    "        # parameter update with adagrad\n",
    "        for param, dparam, mem in zip([self.W_hx, self.W_hh, self.W_hy, self.b_h, self.b_y],\n",
    "                                  [dWxh, dWhh, dWhy, dbh, dby],\n",
    "                                  [self.mWxh, self.mWhh, self.mWhy, self.mbh, self.mby]):\n",
    "            mem += dparam*dparam\n",
    "            param += -learning_rate*dparam/np.sqrt(mem+1e-8) # adagrad update\n",
    "\n",
    "    def train(self, x, y, optimizer=\"sgd\", lr = 1e-3):\n",
    "        \"\"\"The train function which does the following computation in order\n",
    "            1. Calls step(x) i.e traverses through one sequence\n",
    "            2. Gets loss of predicted values from previous step\n",
    "            3. Computes gradients using backward()\n",
    "            4. Updates weight vectors using our choice of optimizer\n",
    "            5. Sets the hidden state to last hidden state\n",
    "\n",
    "        Args:\n",
    "            dW_hx, dW_hh, dW_hy, db_h, db_y: derivatives of weight vectors and bias\n",
    "            optimizer: sgd or adagrad\n",
    "            lr: Learning Rate for SGD\n",
    "        \n",
    "        Returns:\n",
    "            curr_loss: Cross Entropy loss of our step(x)\n",
    "        \"\"\"\n",
    "        loss = 0\n",
    "        x_one_hot, hidden_states, pred_prob = self.step(x)\n",
    "        curr_loss = F.crossentropy(pred_prob, y)\n",
    "        dW_hx, dW_hh, dW_hy, db_h, db_y = self.backward(x_one_hot, y, pred_prob, hidden_states)\n",
    "        if optimizer == \"sgd\":\n",
    "            self.sgd(dW_hx, dW_hh, dW_hy, db_h, db_y, learning_rate=lr)\n",
    "        elif optimizer == \"adagrad\":\n",
    "            self.adagrad(dW_hx, dW_hh, dW_hy, db_h, db_y, learning_rate=lr)\n",
    "            \n",
    "        self.h = hidden_states[len(pred_prob)-1]\n",
    "        return curr_loss\n",
    "\n",
    "    \n",
    "    \n",
    "    def forward(self, seed, n):\n",
    "        \"\"\" \n",
    "        Do one forward pass, with the starting character as seed\n",
    "        h is memory state, seed is seed letter for first time step\n",
    "        \n",
    "        Args:\n",
    "            seed: index of first character for time step\n",
    "            n : size of sequence (time steps)\n",
    "        Returns:\n",
    "            predicted_index: a vector of size n with predicted_indexes for timestep t + 1\n",
    "        \"\"\"\n",
    "        \n",
    "        x = np.zeros((self.input_size, 1))\n",
    "        x[seed] = 1\n",
    "        predicted_indexes = []\n",
    "        for t in range(n):\n",
    "            h = np.copy(self.h)\n",
    "            h = np.tanh(np.dot(self.W_hh, h) + np.dot(self.W_hx, x) + self.b_h)\n",
    "            y = np.dot(self.W_hy, h) + self.b_y\n",
    "            p = F.softmax(y)\n",
    "            predicted_char = np.random.choice(range(self.input_size), p=p.ravel())\n",
    "            x = np.zeros((self.input_size, 1))\n",
    "            x[predicted_char] = 1\n",
    "            predicted_indexes.append(predicted_char)\n",
    "        return predicted_indexes\n",
    "\n",
    "    def show_weight(self):\n",
    "        print(self.h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1000 41.883376713900375\n",
      "CR M sa hwnd abdd lenddiimd ee a ?R2\"ta=/leey a\n",
      "ls\n",
      "0 2000 15.726908296091413\n",
      " ahi nd d 3gw awrre o a!se i aia\n",
      "ekaa\n",
      "d fipnd nd d\n",
      "0 3000 6.1942893340048455\n",
      " ä wiév o  ad tnêe a\n",
      " g o ad Ne ;hXo at adirova\n",
      "a\n",
      "\n",
      "0 4000 2.6596492763197492\n",
      "t adPpt indi oddotoieNe rgi hemt\n",
      " ewd Soaiad l/ ad\n",
      "0 5000 1.3791331478309143\n",
      "ed w ssoo in e ad *i orOw s lb  n End po ebwrero a\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-453-d0d11c43e5fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"adagrad\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0msmooth_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmooth_loss\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.999\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmooth_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-452-805d782be783>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, x, y, optimizer, lr)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mx_one_hot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mcurr_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0mdW_hx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW_hh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW_hy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_one_hot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sgd\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msgd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdW_hx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW_hh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW_hy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-452-805d782be783>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, x, y, p, h, clip_val)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0;31m#backprop into h and through tanh nonlinearity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mdh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_hy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdy\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdh_next\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mdh_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;31m#find updates for h\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sequence_length = 25\n",
    "epochs = 10\n",
    "batches = int(len(data)/sequence_length)\n",
    "rnn = RNN(10, vocab_size)\n",
    "losses = []\n",
    "smooth_loss = -np.log(1.0/vocab_size)*sequence_length\n",
    "losses.append(smooth_loss)\n",
    "data_reader = DataReader(\"warpeace_input.txt\", sequence_length)\n",
    "for epoch in range(epochs):\n",
    "    counter = 0\n",
    "    while(data_reader.epoch_end):\n",
    "        counter += 1\n",
    "        inputs, targets = data_reader.next_batch()\n",
    "        rnn.train(inputs, targets)\n",
    "        if counter%1000 == 999:\n",
    "            pred_x = rnn.forward(inputs[0], 50)\n",
    "            txt_x = ''.join([ix_to_char[n] for n in pred_x])\n",
    "            txt_y = ''.join([ix_to_char[n] for n in y])    \n",
    "            print(epoch, counter+1, losses[-1])\n",
    "            print(txt_x)\n",
    "        loss = rnn.train(x,y,optimizer=\"adagrad\",lr=0.1)\n",
    "        smooth_loss = smooth_loss*0.999 + loss*0.001\n",
    "        losses.append(smooth_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa253ffdd30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(losses)), losses, 'b', label='smooth loss')\n",
    "plt.xlabel('time in thousands of iterations')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
